{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Add main repo to path\n",
    "import sys\n",
    "from os.path import expanduser\n",
    "sys.path.append(expanduser('../../'))\n",
    "\n",
    "from spicy_snow import retrieve_snow_depth\n",
    "from spicy_snow.IO.user_dates import get_input_dates\n",
    "\n",
    "import shapely\n",
    "\n",
    "# Provide bounding box (EPSG:4326 user-provided coordinates)\n",
    "area = shapely.geometry.box(-106.9, 36.9, -106.4, 37.4)\n",
    "\n",
    "# Get tuple of dates. Provided date is ending date and start date is always prior August 1st\n",
    "dates = get_input_dates(\"2022-04-30, 2022-01-01\")\n",
    "\n",
    "# Function to actually get data, run processing, returns xarray dataset w/ daily time dimension\n",
    "#s1_sd = get_s1_snow_depth(area, dates, work_dir = './contrib/data/BPR_retrieval/')\n",
    "# s1_sd = retrieve_snow_depth(area, dates, work_dir = 'Users/Adrian/Desktop/BPR_retrieval/')\n",
    "s1_sd = retrieve_snow_depth(area, dates, work_dir = '.BPR_retrieval')\n",
    "\n",
    "# work_dir will be created if not present \n",
    "# optional keyword ideas: job_name, fitting parameters (A, B, C), exisiting_job_name, outfp\n",
    "# `outfp = './idaho_ret.nc` will output datset to netcdf\n",
    "\n",
    "# plot first day of 2020 to check data quality\n",
    "# s1_sd.sel(time = \"2022-01-01\").plot()\n",
    "\n",
    "# save as pickle file\n",
    "# dump completed dataset to data directory\n",
    "# with open('./BPR_retrieval/spicy_test.pkl', 'wb') as f:\n",
    "    # pickle.dump(ds, f)\n",
    "\n",
    "s1_sd.to_netcdf(f'./BPR_snd2023.nc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<b> NOTES\n",
    "\n",
    "From README file ... </b>\n",
    "\n",
    "Function to actually get data, run processing, returns xarray dataset w/ daily time dimension\n",
    "<br> s1_sd = get_s1_snow_depth(area, dates, work_dir = './idaho_retrieval/)\n",
    "<br>\n",
    "<br> Should 'get_s1_snow_depth' be 'retrieve_snow_depth'?\n",
    "<br>\n",
    "### For future\n",
    "<br> 1. Ability to download multiple years between snow season dates only?\n",
    "<br>\n",
    "<br> 2. If fcf or ims already downloaded, can we use that file instead of redownloading?\n",
    "<br> Skipped for fcf once with exact same search dates/area\n",
    "<br> \n",
    "<br> 3. If error occured in the middle of S1 download, can we start over based on what was completed?\n",
    "<br>\n",
    "<br> 4. More instructions about dates e.g. (End Date, Beginning Date); Has to include at least 2 different retrievals (6-12 date interval minimum) for Snow Index(?)\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spicy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
