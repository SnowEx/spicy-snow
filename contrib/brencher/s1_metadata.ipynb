{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024fbf8f-0fc4-40f7-be5e-61cdb3c91172",
   "metadata": {},
   "source": [
    "## Download and parse scene metadata for flight direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee1387-d044-4471-a932-ef937ed04b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.WARN)\n",
    "import os\n",
    "import getpass\n",
    "import asf_search as asf\n",
    "import shapely\n",
    "import xml.etree.ElementTree as ET\n",
    "import sys\n",
    "from os.path import basename, exists, expanduser, join\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxa\n",
    "from rioxarray.merge import merge_arrays\n",
    "import shapely.geometry\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import hyp3_sdk as sdk\n",
    "from hyp3_sdk.exceptions import AuthenticationError\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48639bd-2929-4787-ac33-89ad28ac1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with NASA Earthdata login to download data\n",
    "EARTHDATA_LOGIN = 'qbrencherUW'\n",
    "EARTHDATA_PASSWORD = getpass.getpass()\n",
    "\n",
    "# prevent DEBUG messages\n",
    "logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
    "\n",
    "try:\n",
    "    user_pass_session = asf.ASFSession().auth_with_creds(EARTHDATA_LOGIN, EARTHDATA_PASSWORD)\n",
    "except asf.ASFAuthenticationError as e:\n",
    "    print(f'Auth failed: {e}')\n",
    "else:\n",
    "    print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1fa95-5670-4402-a46f-f12c45d50130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a persistent .netrc file for downloading NASA datasets\n",
    "!echo \"machine urs.earthdata.nasa.gov login {EARTHDATA_LOGIN} password {EARTHDATA_PASSWORD}\" > ~/.netrc\n",
    "!chmod 0600 ~/.netrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63922a68-ce4c-41f0-9468-163445a577a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set search parameters\n",
    "from shapely.geometry import box\n",
    "area = box(-114.4, 43, -114.3, 43.1)\n",
    "dates = ('2019-12-28', '2020-01-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc008c-6ac7-48ad-824d-2a3f6ea1061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results from asf_search in date range and geometry\n",
    "results = asf.geo_search(platform = [asf.PLATFORM.SENTINEL1], intersectsWith = area.wkt,\n",
    "                         start = dates[0], end = dates[1], processingLevel = asf.PRODUCT_TYPE.GRD_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e4e81-159f-41ba-8e0d-56922e1f9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download scenes\n",
    "data_path = '/home/jovyan/temp_data'\n",
    "results.download(path = data_path, session=user_pass_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45748463-02f6-4bc5-906f-89327fd4fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse xml metadata\n",
    "xml_fn = 'S1A_IW_GRDH_1SDV_20191231T135003_20191231T135028_030593_03813A_A5DB.iso.xml'\n",
    "tree = ET.parse(f'{data_path}/{xml_fn}')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15aff2-2413-4974-98ea-55e86b96fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of namespaces in xml\n",
    "ns = {'gmd':'http://www.isotc211.org/2005/gmd',\n",
    "      'gco':'http://www.isotc211.org/2005/gco',\n",
    "      'xs':'http://www.w3.org/2001/XMLSchema',\n",
    "      'eos':'http://earthdata.nasa.gov/schema/eos',\n",
    "      'echo':'http://www.echo.nasa.gov/ingest/schemas/operatations',\n",
    "      'xlink':'http://www.w3.org/1999/xlink',\n",
    "      'gml':'http://www.opengis.net/gml/3.2',\n",
    "      'gmi':'http://www.isotc211.org/2005/gmi',\n",
    "      'gmx':'http://www.isotc211.org/2005/gmx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab54d1-6833-4534-96db-2a5b87165572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes \n",
    "attributes = {}\n",
    "labels = root.findall('.//eos:EOS_AdditionalAttribute//eos:name/gco:CharacterString', ns)\n",
    "items = root.findall('.//eos:EOS_AdditionalAttribute/eos:value', ns)\n",
    "for i, obj in enumerate(labels):\n",
    "    label = obj.text\n",
    "    item = items[i][0].text\n",
    "    attributes [label] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46822e0f-dfe2-4fa7-bce1-e2ff27f877d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6f8b2-7fb7-4e3b-b32f-129e66054ac3",
   "metadata": {},
   "source": [
    "this method works to reliably get flight direction and other metadata, but it requires downloading and parsing the xml. A faster and easier but potentially more brittle alternative is just to search for the granule name with ascending or descending keywords. The fact that the flight direction is not in the hyp3 metadata is annoying. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f8aaa-8da6-493a-8178-07628648b02a",
   "metadata": {},
   "source": [
    "## Integrate with download functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ada6b-0896-4d0e-9ee2-ba9a1ac6b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(expanduser('/home/jovyan/spicy-snow'))\n",
    "from spicy_snow.utils.download import url_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd3156-9730-4c18-812f-cf7b8546040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s1_img_search(area: shapely.geometry.box, dates: (str, str)) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    find dates and url of Sentinel-1 overpasses\n",
    "\n",
    "    Args:\n",
    "    area: Bounding box of desired area to search within\n",
    "    dates: Start and end date to search between\n",
    "\n",
    "    Returns:\n",
    "    granules: Dataframe of Sentinel-1 granule names to download.\n",
    "    \"\"\"\n",
    "    # Error Checking\n",
    "    if len(dates) != 2:\n",
    "        raise TypeError(\"Provide at start and end date in format (YYYY-MM-DD, YYYY_MM_DD)\")\n",
    "    if type(area) != shapely.geometry.polygon.Polygon:\n",
    "        raise TypeError(\"Geometry must be a shapely.geometry.box type\")\n",
    "    if type(dates[0]) != str:\n",
    "        raise TypeError(\"Provide at start and end date in format (YYYY-MM-DD, YYYY_MM_DD)\")\n",
    "    dates = [pd.to_datetime(d) for d in dates]\n",
    "    if dates[1] < dates[0]:\n",
    "        raise ValueError(\"End date is before start date\")\n",
    "    if dates[0].year < 2017 or dates[1].year < 2017:\n",
    "        raise IndexError(\"Dates are prior to Sentinel-1 launch dates\")\n",
    "    if dates[0].date() > date.today() or dates[1].date() > date.today():\n",
    "        raise IndexError(\"Dates are in the future.\")\n",
    "    if area.bounds[3] > 90 or area.bounds[1] < 0 or area.bounds[2] > 180\\\n",
    "        or area.bounds[0] < -180:\n",
    "        raise IndexError(\"Coordinates must be between 0-90N and -180-180\")\n",
    "\n",
    "    # get results from asf_search in date range and geometry\n",
    "    results = asf.geo_search(platform = [asf.PLATFORM.SENTINEL1], intersectsWith = area.wkt,\\\n",
    "        start = dates[0], end = dates[1], processingLevel = asf.PRODUCT_TYPE.GRD_HD)\n",
    "\n",
    "    # check with 0 results.\n",
    "    if len(results) == 0:\n",
    "        raise ValueError(\"No search results found.\")\n",
    "\n",
    "    # create pandas dataframe from json result\n",
    "    results = pd.json_normalize(results.geojson(), record_path = ['features'])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622b7b5-e17c-477c-a650-fc16f0647b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp3_pipeline(search_results: pd.DataFrame, job_name, existing_job_name = False) -> sdk.jobs.Batch:\n",
    "    \"\"\"\n",
    "    Start and monitor Hyp3 pipeline for desired Sentinel-1 granules\n",
    "    https://hyp3-docs.asf.alaska.edu/using/sdk_api/\n",
    "\n",
    "    Args:\n",
    "    search_results: Pandas Dataframe of asf_search search results.\n",
    "    job_name: name to give hyp3 batch run\n",
    "    existing_job_name: if you have an existing job that you want to find and reuse [default: False]\n",
    "\n",
    "    Returns:\n",
    "    rtc_jobs: Hyp3 batch object of completed jobs.\n",
    "    \"\"\" \n",
    "    try:\n",
    "        # .netrc\n",
    "        hyp3 = sdk.HyP3()\n",
    "    except AuthenticationError:\n",
    "        # prompt for password\n",
    "        hyp3 = sdk.HyP3(prompt = True)\n",
    "\n",
    "    # if existing job name provided then don't submit and simply watch existing jobs.\n",
    "    if existing_job_name:\n",
    "        rtc_jobs = hyp3.find_jobs(name = existing_job_name)\n",
    "        # if no running jobs then just return succeeded jobs\n",
    "        if len(rtc_jobs.filter_jobs(succeeded = False, failed = False)) == 0:\n",
    "            return rtc_jobs.filter_jobs(succeeded = True)\n",
    "        # otherwise watch running jobs\n",
    "        hyp3.watch(rtc_jobs.filter_jobs(succeeded = False, failed = False))\n",
    "        # refresh with new successes and failures\n",
    "        rtc_jobs = hyp3.refresh(rtc_jobs)\n",
    "        # return successful jobs\n",
    "        return rtc_jobs.filter_jobs(succeeded = True)\n",
    "\n",
    "    # gather granules to submit to the hyp3 pipeline\n",
    "    granules = search_results['properties.sceneName']\n",
    "\n",
    "    # create a new hyp3 batch to hold submitted jobs\n",
    "    rtc_jobs = sdk.Batch()\n",
    "    for g in tqdm(granules, desc = 'Submitting s1 jobs'):\n",
    "        # submit rtc jobs and ask for incidence angle map, in amplitude, @ 30 m resolution\n",
    "        # https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job\n",
    "        rtc_jobs += hyp3.submit_rtc_job(g, name = job_name, include_inc_map = True,\\\n",
    "            scale = 'amplitude', dem_matching = False, resolution = 30)\n",
    "\n",
    "    # warn user this may take a few hours for big jobs\n",
    "    print(f'Watching {len(rtc_jobs)}. This may take a while...')\n",
    "\n",
    "    # have hyp3 watch and update progress bar every 60 seconds\n",
    "    hyp3.watch(rtc_jobs)\n",
    "\n",
    "    # refresh jobs list with successes and failures\n",
    "    rtc_jobs = hyp3.refresh(rtc_jobs)\n",
    "\n",
    "    # filter out failed jobs\n",
    "    failed_jobs = rtc_jobs.filter_jobs(succeeded=False, running=False, failed=True)\n",
    "    if len(failed_jobs) > 0:\n",
    "        print(f'{len(failed_jobs)} jobs failed.')\n",
    "    \n",
    "    # return only successful jobs\n",
    "    return rtc_jobs.filter_jobs(succeeded = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11ea92-73f0-49cd-9804-208e955b5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp3_jobs_to_dataArray(jobs: sdk.jobs.Batch, area: shapely.geometry.box, outdir: str, clean = True) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Download rtc Sentinel-1 images from Hyp3 pipeline.\n",
    "    https://hyp3-docs.asf.alaska.edu/using/sdk_api/\n",
    "\n",
    "    Args:\n",
    "    jobs: hyp3 Batch object of completed jobs\n",
    "    outdir: directory to save tif files.\n",
    "    clean: clean up tiffs after creating DataArray [default: True]\n",
    "\n",
    "    Returns:\n",
    "    da: DataArray of Sentinel VV+VH and incidence angle\n",
    "    \"\"\"\n",
    "    # make data directory to store incoming tifs\n",
    "    os.makedirs(outdir, exist_ok = True)\n",
    "    # list to hold new DataArrays from downloaded tiffs\n",
    "    das = []\n",
    "    # list to check if a granule is repeated in the job list\n",
    "    granules = []\n",
    "\n",
    "    for job in tqdm(jobs, desc = 'Downloading S1 images'):\n",
    "        # capture url from job description\n",
    "        u = job.files[0]['url']\n",
    "        # capture granule (for metadata scraping)\n",
    "        granule = job.job_parameters['granules'][0]\n",
    "        # skip this loop if granule is repeated in job list\n",
    "        if granule in granules:\n",
    "            continue\n",
    "        # otherwise append to granules list\n",
    "        granules.append(granule)\n",
    "        # get granule metadata\n",
    "        granule_metadata = asf.product_search(f'{granule}-GRD_HD')[0]\n",
    "        # set flight direction\n",
    "        flight_dir = granule_metadata.properties['flightDirection'].lower()\n",
    "        # set relative orbit \n",
    "        relative_orbit = granule_metadata.properties['pathNumber']\n",
    "        # create dictionary to hold cloud url from .zip url\n",
    "        # this lets us download only VV, VH, inc without getting other data from zip\n",
    "        urls = {}\n",
    "        urls[f'{granule}_VV'] = u.replace('.zip', '_VV.tif')\n",
    "        urls[f'{granule}_VH'] = u.replace('.zip', '_VH.tif')\n",
    "        urls[f'{granule}_inc'] = u.replace('.zip', '_inc_map.tif')\n",
    "        # list to hold each band of image for concating to multi-band image\n",
    "        imgs = []\n",
    "        \n",
    "        for name, url in urls.items():\n",
    "            # download url to a tif file\n",
    "            url_download(url, join(outdir, f'{name}.tif'), verbose = False)\n",
    "            # open image in xarray\n",
    "            img = rxa.open_rasterio(join(outdir, f'{name}.tif'))\n",
    "            # reproject to WGS84\n",
    "            img = img.rio.reproject('EPSG:4326')\n",
    "            # clip to user specified area\n",
    "            img = img.rio.clip([area], 'EPSG:4326')\n",
    "            # add time as a indexable parameter\n",
    "            img = img.assign_coords(time = pd.to_datetime(granule.split('_')[4]))\n",
    "            # add flight direction as indexable parameter\n",
    "            img = img.assign_coords(flight_dir = flight_dir)\n",
    "            # add platform as indexable parameter\n",
    "            platform = granule[0:3]\n",
    "            img = img.assign_coords(platform = platform)\n",
    "            # add relative orbit as indexable parameter\n",
    "            img = img.assign_coords(relative_orbit = relative_orbit)\n",
    "            # create band name\n",
    "            band_name = name.replace(f'{granule}_', '')\n",
    "            # add band to image\n",
    "            imgs.append(img.assign_coords(band = [band_name]))\n",
    "        # concat VV, VH, and inc into one xarray DataArray\n",
    "        da = xr.concat(imgs, dim = 'band')\n",
    "        # we need to reproject each image to match the first image to make CRSs work\n",
    "        if das:\n",
    "            da = da.rio.reproject_match(das[0])\n",
    "        # append multi-band image to das list to concat into time-series DataArray\n",
    "        das.append(da)\n",
    "    # take list of multi-band images with different time values and make time series\n",
    "    full_da = xr.concat(das, dim = 'time')\n",
    "\n",
    "    # remove temp directory of tiffs\n",
    "    if clean:\n",
    "        shutil.rmtree(outdir)\n",
    "    # return the full DataArray of time series multi-band (vv, vh, inc) images clipped to region\n",
    "    return full_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc0866-f2e0-43da-b4c2-c9c097ed5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s1_imgs(search_results: pd.DataFrame, area: shapely.geometry.box, job_name: str = 'sentinel-1-snow-depth', tmp_dir = './tmp', existing_job_name = False) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Download rtc Sentinel-1 images from Hyp3 pipeline.\n",
    "    https://hyp3-docs.asf.alaska.edu/using/sdk_api/\n",
    "\n",
    "    Args:\n",
    "    search_results: Dataframe of asf_search Sentinel-1 granules to download\n",
    "    job_name: job_name to use for hyp3 cloud processing. [default: 'sentinel-1-snow-depth]\n",
    "    tmp_dir: temporary directory to save tifs to\n",
    "\n",
    "    Returns:\n",
    "    s1_dataset: Xarray dataset of Sentinel-1 backscatter and incidence angle\n",
    "    \"\"\"\n",
    "    # submit asf_search results to the hyp3 pipeline and watch for jobs to run\n",
    "    rtc_jobs = hyp3_pipeline(search_results = search_results, job_name = job_name, existing_job_name = existing_job_name)\n",
    "    # download tiffs from successful hyp3 pipeline and convert to the xarray DataArray\n",
    "    s1_dataArray = hyp3_jobs_to_dataArray(jobs = rtc_jobs, area = area, outdir = tmp_dir, clean = False)\n",
    "    # promote to DataSet and set sentinel 1 image to 's1' data variable\n",
    "    s1_dataset = s1_dataArray.to_dataset(name = 's1', promote_attrs = True)\n",
    "    # save to netcdf for testing\n",
    "    # s1_dataset.to_netcdf(out_fp)\n",
    "    return s1_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da256ff-1838-4f33-8483-61dd5bb8b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = s1_img_search(area, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39399f60-5a68-4230-ade6-9e0c41b6dbd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_da = download_s1_imgs(results, area, tmp_dir = '/tmp/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44f259-ca6a-43ef-80da-33eddd350c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.local-spicy]",
   "language": "python",
   "name": "conda-env-.local-spicy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
